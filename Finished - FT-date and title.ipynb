{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from Financial Times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This data was scraped august 18, 2017, 12:00.\n",
    "- It is the result of a search on 'Bitcoin'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports data for our webscra\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find article-titles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find kkk (all-title-list ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The ‘finance franchise’ and fintech (Part 2)', 'Goldman’s sketchy case to buy (and then sell) bitcoin', 'HBO hacked again in Twitter accounts breach', 'Australia follows Japan in move to regulate bitcoin', 'Markets Live: Tuesday, 15th August, 2017', 'Bitcoin above $4,000 in record run for cryptocurrency', 'When altcoin life imitates art', 'Markets Live: Monday, 14th August, 2017', 'Bitcoin holds above record $4,000 level', 'Bitcoin holds above record $4,000 level', 'Escalating US-North Korea spat remains in focus', 'Why small change still matters to millionaires', 'Rich take out home loans to gamble on markets', 'Daily briefing:\\xa0Mueller investigation intensifies, WannaCry nemesis arrested, how to launder $81m', 'Daily briefing: Trump moves to slash immigration, UK chief executive pay falls, female bosses', 'Further reading', 'Rival bitcoin currency has volatile trading debut', 'CBOE to launch bitcoin future contracts', 'Daily briefing: Trump on China, iPhone revival, the bitcoin fork', 'Further reading', 'Bitcoin splits into two transaction as volumes rise', 'Crypto currencies are mirroring pre-crash banking systems', 'Why there is no such thing as a trustless financial system', 'Imagine if you received this note from your bank?', 'Initial coin offerings: tales from the cryptos', 'Attack of the 50-foot blockchain, a sceptic’s guide to crypto', 'The huge significance of the BTC-e bust', 'Further reading', 'A $4bn bitcoin laundering operation potentially busted', 'Cryptocurrency exchanges could be subject to SEC regulation, too', 'SEC looks to deflate ‘Initial Coin Offerings’ bubble', 'SEC says initial coin offerings may be subject to securities law', 'Snap AV: Bitcoin is the ultimate diversification tool…', 'Dark web master thwarted by love of old-fashioned cash', 'Stephen Barclay impresses with his Brexit nous', 'Global cyber sting shuts down dark web bazaars', 'Next stop, decentralised criminal marketplaces..?', 'ICOs now take Visa (plus other ingenious solicitation temptations!)', 'Crypto currency craze is a bubble in the making', 'ICOs and the money markets', 'Tech start-ups raise $1.3bn from initial coin offerings', 'Authers’ Note: Restoration Comedy', 'Markets Live: Monday, 17th July, 2017', 'Further reading', 'Dark Net may pose ‘disruptive risk’ to internet sector – Goldman', 'Further reading', 'BoE successfully tests new payment method', 'Mt Gox former chief pleads not guilty to embezzlement', 'Crypto frother warns of crypto froth (while further frothing crypto)', 'Worldpay emerges as a winner in the war on cash']\n"
     ]
    }
   ],
   "source": [
    "#This function makes a list with all pages containing search results.\n",
    "def next_page():\n",
    "    url = 'http://www.ft.com/search?q=bitcoin&page='\n",
    "    FTpages= []\n",
    "    for i in range(0,47):\n",
    "        nextpage =  url + str(i)      \n",
    "        FTpages.append(nextpage)\n",
    "    return FTpages\n",
    "\n",
    "\n",
    "# Call funktion and subscribe list from function to variable\n",
    "FTpages = next_page()\n",
    "\n",
    "\n",
    "titles=[]\n",
    "# Choose one page-http: adress\n",
    "for pagenr in FTpages:\n",
    "# pick text from page-http:\n",
    "    response = requests.get(pagenr)\n",
    "# Pick the text in another format \n",
    "    soup = BeautifulSoup(response.text,\"lxml\" )\n",
    "# Finds substring of HTML including the title\n",
    "    page = soup.find_all('a', attrs={'class':'js-teaser-heading-link'})\n",
    "# makes a list with all links (and some other text) included \n",
    "    for links in page:\n",
    "        titles.append(str(links))\n",
    "# extract only the links from the titles-list and name the all-title list - kkk.  \n",
    "kkk=[i.split('>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',1)[1].split('\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',1)[0] for i in titles]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unicode from kkk (all-title-list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'You say you want a revolution\\u2009.\\u2009.\\u2009.\\u2009' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-91bb318214f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# First unicode-characters removed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkkk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You say you want a revolution\\u2009.\\u2009.\\u2009.\\u2009\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mkkk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You say you want a revolution\\u2009.\\u2009.\\u2009.\\u2009\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mkkk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m787\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"You say you want a revolution\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'You say you want a revolution\\u2009.\\u2009.\\u2009.\\u2009' is not in list"
     ]
    }
   ],
   "source": [
    "# First unicode-characters removed\n",
    "kkk.index(\"You say you want a revolution\\u2009.\\u2009.\\u2009.\\u2009\")\n",
    "kkk.remove(\"You say you want a revolution\\u2009.\\u2009.\\u2009.\\u2009\")\n",
    "kkk.insert(787,\"You say you want a revolution\")\n",
    "\n",
    "# Second unicode-character removed\n",
    "kkk.index(\"Similar immunity of Bitcoin\\u200a and\\u200a gold\")\n",
    "kkk.remove('Similar immunity of Bitcoin\\u200a and\\u200a gold')\n",
    "kkk.insert(900,\"Similar immunity of Bitcoin and gold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all-title-list as csv.file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"titel1.csv\", \"w\") as out_file: \n",
    "    for i in range(len(kkk)):\n",
    "        out_string = \"\"\n",
    "        out_string += str(kkk[i])\n",
    "        out_string += '\\n'\n",
    "        out_file.write(out_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find article-dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find jjj(all-dates-list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.ft.com/search?q=bitcoin&page=0', 'http://www.ft.com/search?q=bitcoin&page=1', 'http://www.ft.com/search?q=bitcoin&page=2']\n",
      "['Friday, 18 August, 2017', 'Friday, 18 August, 2017', 'Thursday, 17 August, 2017', 'Thursday, 17 August, 2017', 'Thursday, 17 August, 2017', 'Tuesday, 15 August, 2017', 'Monday, 14 August, 2017', 'Monday, 14 August, 2017', 'Monday, 14 August, 2017', 'Monday, 14 August, 2017', 'Monday, 14 August, 2017', 'Sunday, 13 August, 2017', 'Sunday, 13 August, 2017', 'Sunday, 6 August, 2017', 'Friday, 4 August, 2017', 'Thursday, 3 August, 2017', 'Thursday, 3 August, 2017', 'Wednesday, 2 August, 2017', 'Wednesday, 2 August, 2017', 'Wednesday, 2 August, 2017', 'Wednesday, 2 August, 2017', 'Tuesday, 1 August, 2017', 'Tuesday, 1 August, 2017', 'Monday, 31 July, 2017', 'Friday, 28 July, 2017', 'Friday, 28 July, 2017', 'Thursday, 27 July, 2017', 'Thursday, 27 July, 2017', 'Thursday, 27 July, 2017', 'Wednesday, 26 July, 2017', 'Wednesday, 26 July, 2017', 'Wednesday, 26 July, 2017', 'Tuesday, 25 July, 2017', 'Tuesday, 25 July, 2017', 'Saturday, 22 July, 2017', 'Friday, 21 July, 2017', 'Thursday, 20 July, 2017', 'Thursday, 20 July, 2017', 'Wednesday, 19 July, 2017', 'Tuesday, 18 July, 2017', 'Tuesday, 18 July, 2017', 'Tuesday, 18 July, 2017', 'Monday, 17 July, 2017', 'Monday, 17 July, 2017', 'Monday, 17 July, 2017', 'Thursday, 13 July, 2017', 'Thursday, 13 July, 2017', 'Tuesday, 11 July, 2017', 'Tuesday, 11 July, 2017', 'Monday, 10 July, 2017']\n"
     ]
    }
   ],
   "source": [
    "# Call funktion and subscribe list from function to variable\n",
    "FTpages = next_page()\n",
    "print(FTpages)\n",
    "\n",
    "dates=[]\n",
    "for pagenr in FTpages:\n",
    "# pick up text from the first page of the Bitcoin search\n",
    "    response = requests.get(pagenr)\n",
    "# Pick the text in another format \n",
    "    soup = BeautifulSoup(response.text,\"lxml\" )\n",
    "# Finds substring of HTML including the date\n",
    "    page = soup.find_all('div', attrs={'class':'stream-card__date'})\n",
    "# makes a list with all dates (and some other text) included \n",
    "    for links in page:\n",
    "        dates.append(str(links))    \n",
    "# extract only the date from the dates-list and name the all-date-list - jjj.    \n",
    "jjj=[i.split('000Z\">',1)[1].split('</time>',1)[0] for i in dates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all-dates-list as csv.file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"dato1.csv\", \"w\") as out_file: \n",
    "    for i in range(len(jjj)):\n",
    "        out_string = \"\"\n",
    "        out_string += str(jjj[i])\n",
    "        out_string += '\\n'\n",
    "        out_file.write(out_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks - Datascrape Financial Times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to have fully matched title and dates, we have made a few manually changes to the csv-files and merged them into one common file.\n",
    "- From this point we will just import the data into two lists and make a dataframe.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
